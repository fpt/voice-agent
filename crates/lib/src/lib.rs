pub mod capture;
pub mod event_router;
mod harmony;
mod llm;
#[cfg(feature = "local")]
pub mod llm_local;
pub mod mcp;
pub mod mcp_client;
pub mod mcp_client_http;
pub mod mcp_server;
pub mod mcp_server_http;
mod memory;
pub mod react;
pub mod situation;
pub mod skill;
mod state_updater;
pub mod tool;

use parking_lot::Mutex;
use std::path::PathBuf;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;

pub use capture::CaptureRequest;
pub use harmony::HarmonyTemplate;
pub use llm::{create_provider, ChatMessage, ChatRole, TokenUsage};
use tool::ToolAccess;
pub use memory::ConversationMemory;
pub use state_updater::{BackchannelDetector, RuleBasedBackchannelDetector};

// UniFFI generated code
uniffi::include_scaffolding!("agent");

/// JSON Schema for keyword extraction
fn get_keyword_schema() -> serde_json::Value {
    serde_json::json!({
        "type": "object",
        "properties": {
            "response": {
                "type": "string",
                "description": "Your natural language response to the user"
            },
            "keywords": {
                "type": "array",
                "description": "Important keywords from this conversation for speech recognition context (proper nouns, technical terms, domain-specific words)",
                "items": {
                    "type": "string"
                },
                "maxItems": 10
            }
        },
        "required": ["response", "keywords"],
        "additionalProperties": false
    })
}

/// Parse structured JSON response containing both response text and keywords
fn parse_structured_response(json_str: &str) -> Result<(String, Vec<String>), AgentError> {
    let parsed: serde_json::Value = serde_json::from_str(json_str)
        .map_err(|e| AgentError::ParseError(format!("Failed to parse JSON: {}", e)))?;

    let response = parsed["response"]
        .as_str()
        .ok_or_else(|| AgentError::ParseError("Missing 'response' field".to_string()))?
        .to_string();

    let keywords = parsed["keywords"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                .collect()
        })
        .unwrap_or_default();

    Ok((response, keywords))
}

/// Configuration for an external MCP server to spawn and connect to.
pub struct McpServerConfig {
    pub command: String,
    pub args: Vec<String>,
}

/// Configuration for the agent
pub struct AgentConfig {
    pub model_path: Option<String>,
    pub base_url: String,
    pub model: String,
    pub api_key: Option<String>,
    pub use_harmony_template: bool,
    pub temperature: Option<f32>,
    pub max_tokens: u32,
    /// Model context window size in tokens (used for compaction triggering).
    pub context_window: u32,
    pub language: Option<String>,
    pub working_dir: Option<String>,
    pub reasoning_effort: Option<String>,
    pub mcp_servers: Vec<McpServerConfig>,
}

impl Default for AgentConfig {
    fn default() -> Self {
        Self {
            model_path: None,
            base_url: "https://api.openai.com/v1".to_string(),
            model: "gpt-4o-mini".to_string(),
            api_key: None,
            use_harmony_template: true,
            temperature: Some(0.7),
            max_tokens: 2048,
            context_window: 128_000,
            language: Some("en".to_string()),
            working_dir: None,
            reasoning_effort: None,
            mcp_servers: Vec::new(),
        }
    }
}

/// Response from the agent
pub struct AgentResponse {
    pub content: String,
    pub role: String,
    pub is_final: bool,
    pub keywords: Option<Vec<String>>,
    pub reasoning: Option<String>,
    pub input_tokens: u64,
    pub output_tokens: u64,
    pub total_tokens: u64,
    pub context_percent: f32,
}

/// Error types for the agent
#[derive(Debug, thiserror::Error)]
pub enum AgentError {
    #[error("Network error: {0}")]
    NetworkError(String),
    #[error("Parse error: {0}")]
    ParseError(String),
    #[error("Configuration error: {0}")]
    ConfigError(String),
    #[error("Internal error: {0}")]
    InternalError(String),
}

/// Main agent struct
pub struct Agent {
    config: AgentConfig,
    client: Box<dyn llm::LlmProvider>,
    memory: Arc<Mutex<ConversationMemory>>,
    backchannel_detector: Box<dyn BackchannelDetector>,
    system_prompt: Arc<Mutex<Option<String>>>,
    tool_registry: tool::ToolRegistry,
    skill_registry: Arc<skill::SkillRegistry>,
    situation: Arc<situation::SituationMessages>,
    last_input_tokens: AtomicU64,
    capture_request_rx: crossbeam::channel::Receiver<capture::CaptureRequest>,
    capture_result_tx: crossbeam::channel::Sender<capture::CaptureResult>,
    find_result_tx: crossbeam::channel::Sender<capture::CaptureResult>,
    ocr_result_tx: crossbeam::channel::Sender<capture::CaptureResult>,
}

// Top-level constructor function for UniFFI
pub fn agent_new(config: AgentConfig) -> Result<Arc<Agent>, AgentError> {
    // Initialize tracing (only once)
    let _ = tracing_subscriber::fmt()
        .with_env_filter(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| tracing_subscriber::EnvFilter::new("info")),
        )
        .try_init();

    // Create LLM provider
    let client = create_provider(
        config.model_path.clone(),
        config.base_url.clone(),
        config.model.clone(),
        config.api_key.clone(),
        config.temperature,
        config.max_tokens,
        config.reasoning_effort.clone(),
    )
    .map_err(|e| AgentError::ConfigError(e.to_string()))?;

    // Create tool registry with built-in tools
    let working_dir = config
        .working_dir
        .as_ref()
        .map(PathBuf::from)
        .unwrap_or_else(|| std::env::current_dir().unwrap_or_else(|_| PathBuf::from(".")));

    tracing::info!("Tool working directory: {}", working_dir.display());
    let skill_registry = Arc::new(skill::SkillRegistry::new());

    let situation = Arc::new(situation::SituationMessages::default());

    // Create capture bridge
    let capture_bridge = capture::CaptureBridge::new();

    let mut tool_registry = tool::create_default_registry(
        working_dir,
        skill_registry.clone(),
        situation.clone(),
    );

    // Connect to configured MCP servers and register their tools
    for server_cfg in &config.mcp_servers {
        let args_ref: Vec<&str> = server_cfg.args.iter().map(|s| s.as_str()).collect();
        match mcp_client::McpClient::connect(&server_cfg.command, &args_ref) {
            Ok(client) => {
                for handler in client.tool_handlers() {
                    tool_registry.register(handler);
                }
            }
            Err(e) => {
                tracing::warn!("Failed to connect MCP server '{}': {}", server_cfg.command, e);
            }
        }
    }

    // Register capture tools (shared request channel, separate result channels)
    tool_registry.register(Box::new(capture::CaptureScreenTool::new(
        capture_bridge.request_tx.clone(),
        capture_bridge.capture_result_rx.clone(),
    )));
    tool_registry.register(Box::new(capture::FindWindowTool::new(
        capture_bridge.request_tx.clone(),
        capture_bridge.find_result_rx.clone(),
    )));
    tool_registry.register(Box::new(capture::ApplyOcrTool::new(
        capture_bridge.request_tx.clone(),
        capture_bridge.ocr_result_rx.clone(),
    )));

    Ok(Arc::new(Agent {
        config,
        client,
        memory: Arc::new(Mutex::new(ConversationMemory::new())),
        backchannel_detector: Box::new(RuleBasedBackchannelDetector::new()),
        system_prompt: Arc::new(Mutex::new(None)),
        tool_registry,
        skill_registry,
        situation,
        last_input_tokens: AtomicU64::new(0),
        capture_request_rx: capture_bridge.request_rx,
        capture_result_tx: capture_bridge.capture_result_tx,
        find_result_tx: capture_bridge.find_result_tx,
        ocr_result_tx: capture_bridge.ocr_result_tx,
    }))
}

impl Agent {
    /// Process a user input and return the agent's response
    pub fn step(&self, user_input: String) -> Result<AgentResponse, AgentError> {
        let mut memory = self.memory.lock();

        // Compact if last turn approached context window limit (>= 90%)
        self.maybe_compact(&mut memory);

        // Add user message to memory
        memory.add_message(ChatMessage::user(user_input.clone()));

        // Get conversation context
        let mut messages = memory.get_messages();

        // Prepend custom system prompt if set
        let system_prompt = self.system_prompt.lock().clone();
        if let Some(prompt) = system_prompt {
            messages.insert(0, ChatMessage::system(prompt));
        }

        // Inject skill catalog so LLM knows what skills are available
        if let Some(catalog) = self.skill_registry.catalog() {
            messages.push(ChatMessage::system(catalog));
        }

        // Apply Harmony template if enabled
        let formatted_messages = if self.config.use_harmony_template {
            HarmonyTemplate::format_messages(&messages)
        } else {
            messages.clone()
        };

        // Use ReAct loop if provider supports tools and tools are registered
        let (response_text, keywords, reasoning, usage) = if self.client.supports_tools()
            && !self.tool_registry.is_empty()
        {
            // ReAct loop with tool calling
            let mut react_messages = formatted_messages;
            let (text, reasoning, usage) = react::run(
                self.client.as_ref(),
                &mut react_messages,
                &self.tool_registry,
                None,
            )?;

            (text, Vec::new(), reasoning, usage)
        } else if self.client.supports_structured_output() {
            // Structured output for keyword extraction (no tools)
            let schema = get_keyword_schema();
            let json_response = self
                .client
                .chat_with_schema(&formatted_messages, schema, "conversation_response")
                .map_err(|e| AgentError::NetworkError(e.to_string()))?;
            let (text, keywords) = parse_structured_response(&json_response)?;
            (text, keywords, None, TokenUsage::default())
        } else {
            // Fallback: regular chat (no keywords, no tools)
            let response = self
                .client
                .chat(&formatted_messages)
                .map_err(|e| AgentError::NetworkError(e.to_string()))?;
            (response, Vec::new(), None, TokenUsage::default())
        };

        // Track token usage for compaction decisions
        self.last_input_tokens.store(usage.input_tokens, Ordering::Relaxed);

        // Add assistant response to memory
        memory.add_message(ChatMessage::assistant(response_text.clone()));

        let context_percent = if self.config.context_window > 0 {
            (usage.input_tokens as f64 / self.config.context_window as f64 * 100.0) as f32
        } else {
            0.0
        };

        Ok(AgentResponse {
            content: response_text,
            role: "assistant".to_string(),
            is_final: true,
            keywords: if keywords.is_empty() { None } else { Some(keywords) },
            reasoning,
            input_tokens: usage.input_tokens,
            output_tokens: usage.output_tokens,
            total_tokens: usage.total_tokens,
            context_percent,
        })
    }

    /// Process a backchannel event (audio only, no history pollution)
    pub fn process_backchannel(&self, partial_input: String, pause_ms: u64) -> Option<String> {
        if let Some(backchannel_text) = self
            .backchannel_detector
            .should_backchannel(&partial_input, pause_ms)
        {
            let mut memory = self.memory.lock();
            memory.add_backchannel();
            tracing::debug!("Backchannel triggered: '{}'", backchannel_text);
            return Some(backchannel_text);
        }
        None
    }

    /// Reset the conversation memory
    pub fn reset(&self) {
        let mut memory = self.memory.lock();
        memory.clear();
    }

    /// Get the conversation history as JSON string
    pub fn get_conversation_history(&self) -> String {
        let memory = self.memory.lock();
        serde_json::to_string_pretty(&memory.get_messages()).unwrap_or_default()
    }

    /// Set a custom system prompt for the conversation
    pub fn set_system_prompt(&self, prompt: String) {
        let mut system_prompt = self.system_prompt.lock();
        *system_prompt = Some(prompt);
        tracing::info!("System prompt set");
    }

    /// Register a skill with the agent
    pub fn add_skill(&self, name: String, description: String, prompt: String) {
        self.skill_registry.add(name, description, prompt);
    }

    /// Process user input with only a subset of tools enabled
    pub fn step_with_allowed_tools(
        &self,
        user_input: String,
        allowed_tools: Vec<String>,
    ) -> Result<AgentResponse, AgentError> {
        let mut memory = self.memory.lock();

        // Compact if last turn approached context window limit (>= 90%)
        self.maybe_compact(&mut memory);

        // Add user message to memory
        memory.add_message(ChatMessage::user(user_input.clone()));

        // Get conversation context
        let mut messages = memory.get_messages();

        // Prepend custom system prompt if set
        let system_prompt = self.system_prompt.lock().clone();
        if let Some(prompt) = system_prompt {
            messages.insert(0, ChatMessage::system(prompt));
        }

        // Inject skill catalog
        if let Some(catalog) = self.skill_registry.catalog() {
            messages.push(ChatMessage::system(catalog));
        }

        // Apply Harmony template if enabled
        let formatted_messages = if self.config.use_harmony_template {
            HarmonyTemplate::format_messages(&messages)
        } else {
            messages.clone()
        };

        // Use ReAct loop with filtered tools
        let filtered = self.tool_registry.filtered(&allowed_tools);
        let (response_text, keywords, reasoning, usage) = if self.client.supports_tools()
            && !filtered.is_empty()
        {
            let mut react_messages = formatted_messages;
            let (text, reasoning, usage) = react::run(
                self.client.as_ref(),
                &mut react_messages,
                &filtered,
                None,
            )?;
            (text, Vec::new(), reasoning, usage)
        } else if self.client.supports_structured_output() {
            let schema = get_keyword_schema();
            let json_response = self
                .client
                .chat_with_schema(&formatted_messages, schema, "conversation_response")
                .map_err(|e| AgentError::NetworkError(e.to_string()))?;
            let (text, keywords) = parse_structured_response(&json_response)?;
            (text, keywords, None, TokenUsage::default())
        } else {
            let response = self
                .client
                .chat(&formatted_messages)
                .map_err(|e| AgentError::NetworkError(e.to_string()))?;
            (response, Vec::new(), None, TokenUsage::default())
        };

        // Track token usage for compaction decisions
        self.last_input_tokens.store(usage.input_tokens, Ordering::Relaxed);

        // Add assistant response to memory
        memory.add_message(ChatMessage::assistant(response_text.clone()));

        let context_percent = if self.config.context_window > 0 {
            (usage.input_tokens as f64 / self.config.context_window as f64 * 100.0) as f32
        } else {
            0.0
        };

        Ok(AgentResponse {
            content: response_text,
            role: "assistant".to_string(),
            is_final: true,
            keywords: if keywords.is_empty() { None } else { Some(keywords) },
            reasoning,
            input_tokens: usage.input_tokens,
            output_tokens: usage.output_tokens,
            total_tokens: usage.total_tokens,
            context_percent,
        })
    }

    /// Feed a watcher event â€” parses JSON and pushes to the situation stack.
    /// The LLM can read these via the read_situation_messages tool when the user asks.
    pub fn feed_watcher_event(&self, json: String) -> Result<(), AgentError> {
        let event: event_router::WatcherEvent = serde_json::from_str(&json)
            .map_err(|e| AgentError::ParseError(format!("Invalid event JSON: {}", e)))?;
        if let Some((line, source, session_id)) = format_event_for_situation(&event) {
            self.situation.push(line, source, session_id);
        }
        Ok(())
    }

    /// Drain all pending capture requests (Swift polls this).
    pub fn drain_capture_requests(&self) -> Vec<capture::CaptureRequest> {
        let mut requests = Vec::new();
        while let Ok(req) = self.capture_request_rx.try_recv() {
            requests.push(req);
        }
        requests
    }

    /// Submit a capture result from Swift back to the waiting Rust tool.
    /// Routes to the correct channel based on request ID prefix.
    pub fn submit_capture_result(
        &self,
        id: String,
        image_base64: String,
        metadata_json: String,
    ) {
        let result = capture::CaptureResult {
            id: id.clone(),
            image_base64,
            metadata_json,
        };
        if id.starts_with("find_") {
            let _ = self.find_result_tx.send(result);
        } else if id.starts_with("ocr_") {
            let _ = self.ocr_result_tx.send(result);
        } else {
            let _ = self.capture_result_tx.send(result);
        }
    }

    /// Compact memory if the last turn's input tokens reached >= 90% of context window.
    /// Targets 50% of context window after compaction to leave room.
    fn maybe_compact(&self, memory: &mut ConversationMemory) {
        let last = self.last_input_tokens.load(Ordering::Relaxed);
        if last == 0 {
            return;
        }
        let threshold = (self.config.context_window as f64 * 0.9) as u64;
        if last >= threshold {
            let target = self.config.context_window as usize / 2;
            let dropped = memory.compact(target);
            if dropped > 0 {
                tracing::info!(
                    "Compacted memory: dropped {} messages (last input: {} tokens, window: {})",
                    dropped,
                    last,
                    self.config.context_window,
                );
            }
        }
    }

    /// Push a situation message from Swift (e.g. periodic window list).
    pub fn push_situation_message(&self, text: String, source: String, session_id: String) {
        self.situation.push(text, source, session_id);
    }
}

/// Extract the last path component for display.
fn path_basename(path: &str) -> &str {
    std::path::Path::new(path)
        .file_name()
        .and_then(|f| f.to_str())
        .unwrap_or(path)
}

/// Format a WatcherEvent as a one-line situation message.
/// Returns `(line, source, session_id)` or `None` for events that shouldn't appear.
///
/// Lines are prefixed with `[Claude Code <project>]` so the LLM knows the source.
fn format_event_for_situation(
    event: &event_router::WatcherEvent,
) -> Option<(String, String, String)> {
    match event {
        event_router::WatcherEvent::Hook(h) => {
            let session_id = h.session_id.clone().unwrap_or_default();
            let project = path_basename(&session_id);
            let detail = if let Some(ref tool) = h.tool_name {
                if let Some(ref path) = h.file_path {
                    format!("{}: {}", tool, path_basename(path))
                } else {
                    tool.clone()
                }
            } else {
                h.event.clone()
            };
            let line = format!("[Claude Code {}] {}", project, detail);
            Some((line, "hook".to_string(), session_id))
        }
        event_router::WatcherEvent::Session(s) => {
            if s.tool_uses.is_empty() {
                return None;
            }
            let session_id = s.session_id.clone().unwrap_or_default();
            let project = path_basename(&session_id);
            let tools: Vec<&str> = s.tool_uses.iter().map(|t| t.name.as_str()).collect();
            let line = format!("[Claude Code {}] {}: {}", project, s.event_type, tools.join(", "));
            Some((line, "session".to_string(), session_id))
        }
        event_router::WatcherEvent::UserSpeech(_) => None, // goes to main conversation
    }
}
